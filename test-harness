#!/bin/bash

# Testing Shell Kit

# Guiding Principles
#
#  - Tests are meant to look and act like bash code. Familiarity is key.
#  - Testing is as self-contained as possible, nothing to install on the host.
#  - Test output should contain enough information to see what went wrong.
#  - the API should be clean and readable; no need to use things named _ or tsk_.
#
# Tests are run with an empty writeable directory as the CWD. They
# are expected to leave this directory empty when they're finished,
# otherwise the test fails. TODO
#
# If your test produces output on stderr, make sure to check it,
# otherwise the test will fail. If you want to
# ignore stderr, you can do something like TODO: `stderr_matches .`
#

#: ## The Test Environment
#:
#: ### directories
#:
#: When a test is running, the current working directory is set
#: to an empty scratch directory. The test should make no assumption
#: about where this directory is stored; it might be in your project
#: directory, it might be in /tmp, and it might be on a magic ramdisk
#: somewhere.
#:
#: Your test can also access these directories:
#: - testfile_dir: The directory containing the testfile.
#:   For example, if you
#:   have some sample data in a file next to the testfile, you can
#:   specify it like this: `$(testfile_dir)/sample_data.txt`
#: - framework_dir: The directory containing the run-tests script.
#:   If you have a helper script in the same directory as the run-tests
#:   script, you can include it like this: `$(framework_dir)/helper.sh`
#:
#: ## The Testfile
#:
#: Testfiles start by including the test framework:
#:
#: ```bash
#: source "$(dirname "$0")/../run-tests.sh"
#: ```



# we reserve the tsk_ prefix to prevent incurring any conflicts
# with variables in the tests themselves. TODO: can we test if we
# accidentally leak any variables or non-api functions into the testfiles?

# Store the path to the run-tests.sh script for later use
tsk_framework_path="$(readlink -f "${BASH_SOURCE[0]}")"
tsk_testfile_path=""  # the testfile that we're interpreting

# Set the testfile path when this script is sourced
if [ "${BASH_SOURCE[1]}" != "" ]; then
    tsk_testfile_path="$(readlink -f "${BASH_SOURCE[1]}")"
fi

# save the environment so it can be restored at the end
tsk_orig_cwd=""       # original directory before testing started
tsk_orig_path=""      # original PATH before testing started
exec 3>&1 4>&2        # copy stdout and stderr to FDs 3 and 4

tsk_total_count=0     # current number of tests run
tsk_pass_count=0      # current number of tests passed
tsk_fail_count=0      # current number of tests failed
tsk_skip_count=0      # current number of tests skipped
tsk_total_assertions=0 # total number of assertions made
tsk_root_dir=""       # main test directory
tsk_run_dir=""        # directory where tests run
tsk_artifacts_dir=""  # directory for temporary test artifacts

# these variables are reset every time a new test is run
tsk_test_name=""      # the name of the test currently being run
declare -a tsk_test_messages=() # array containing a message for each error
tsk_test_assertions=0 # current number of assertions made in the test
tsk_stderr_checked=false # whether stderr has been explicitly checked

# TODO: this is a misnomer... we also use tsk_use_color to indicate
# if we should use control characters to do better terminal output
if [ -z "${tsk_use_color}" ]; then
    tsk_use_color=false
    if [ -t 3 ]; then
        tsk_use_color=true
    fi
fi

# Colors for output
# TODO: get these colors out of the test global scope
if [ "$tsk_use_color" = true ]; then
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color
    CR=$'\r'
else
    RED=''
    GREEN=''
    YELLOW=''
    NC=''
    CR=''
fi

# save stdout and stderr so we can restore them after testing
exec 3>&1 4>&2


#
#    The API available to tests...
#

is_eq() {
    local expected=$1
    local actual=$2

    tsk_test_assertions=$((tsk_test_assertions + 1))

    if [ "$expected" != "$actual" ]; then
        _add_error "line $(_line_number): is_eq expected '$expected', got '$actual'"
    fi
}

stderr_contains() {
    local expected_text="$1"

    tsk_test_assertions=$((tsk_test_assertions + 1))
    tsk_stderr_checked=true

    if ! grep -q "$expected_text" "$tsk_artifacts_dir/test-stderr"; then
        _add_error_with_stderr "stderr_contains line $(_line_number): stderr doesn't contain: '$expected_text'"
    fi
}

# One trailing newline, if present, is trimmed from stdout before comparison.
# If you need to be pedantically correct about the presence of the final
# newline, you'll have to check for it outside of this function.
# TODO: this is not great.
file_is() {
    local expected_text="$1"
    local file_path="$2"
    local call_name="${3:-file_is}"
    local lineno="${4:-$(_line_number)}"

    tsk_test_assertions=$((tsk_test_assertions + 1))

    if [ -z "$file_path" ] || [ ! -f "$file_path" ]; then
        _add_error "line $lineno: $call_name couldn't open '$file_path'"
        return
    fi

    local actual_text="$(cat "$file_path")"
    if [ "$expected_text" != "$actual_text" ]; then
        _add_error "line $lineno: $call_name $(_format_diff "$expected_text" "$actual_text")"
    fi
}

stdout_is() {
    file_is "$1" "$tsk_artifacts_dir/test-stdout" stdout_is "$(_line_number)"
}

stderr_is() {
    file_is "$1" "$tsk_artifacts_dir/test-stderr" stderr_is "$(_line_number)"
    tsk_stderr_checked=true
}


abort() {
    # TODO: should run tests in a subshell that keeps testing
    # until abort is called. Abort exits the subshell.
    # Potentially skip could use this technique too but that's
    # probably not worth it.
    local errmsg="line $(_line_number): $1"
    printf "${CR} %02d ${RED}ABORT${NC}  $tsk_test_name $errmsg\n" "$tsk_total_count" >&3
    printf "${RED}TEST ABORTED${NC}\n" >&3
    exit 127
}

mock() {
    local command="$1"
    local behavior="$2"
    local mock_dir="$tsk_artifacts_dir/test-mocks"
    local executable="$mock_dir/$command"

    # Create the mock directory and add it to PATH if needed
    if [ ! -d "$mock_dir" ]; then
        mkdir -p "$mock_dir"
    fi

    # Ensure we have an absolute path for the mock directory
    mock_dir="$(readlink -f "$mock_dir")"
    executable="$mock_dir/$command"

    # Add mock dir to PATH only when the first mock is created
    if [[ ":$PATH:" != *":$mock_dir:"* ]]; then
        export PATH="$mock_dir:$PATH"
    fi

    cat > "$executable" << EOF
#!/bin/bash
$behavior
EOF
    chmod +x "$executable"
}

# Returns the directory of the currently running testfile
testfile_dir() {
    if [ -z "$tsk_testfile_path" ]; then
        echo "Error: No testfile detected. THIS IS A BUG. exiting now." >&4
        exit 1
    fi
    dirname "$tsk_testfile_path"
}

# Returns the directory of the test-harness script
framework_dir() {
    dirname "$tsk_framework_path"
}

# call this to make differences be displayed using `diff` rather than side-by-side
want_diff() {
    tsk_test_want_format="diff"
}

pluralize() {
    local count=$1
    local singular=$2
    local plural=${3:-${singular}s}

    if [ "$count" -eq 1 ]; then
        echo "$singular"
    else
        echo "$plural"
    fi
}


#
#    Testing internals
#

# returns the line number of the caller of the given frame
# defaults to the caller of the function calling _line_number
_line_number() {
    local frame=${1:-1}
    local info=$(caller "$frame")
    echo "${info%% *}"
}

# used to quote large content in the test output, like stderr
_blockquote() {
    echo -n "$1" | sed 's/^/    | /'
}

_format_diff() {
    local expected="$1"
    local actual="$2"

    # For short single-line content, always use simple quoted format
    if [[ "$expected" != *$'\n'* && "$actual" != *$'\n'* &&
          ${#expected} -lt 30 && ${#actual} -lt 30 ]]; then
        echo "expected '$expected', got '$actual'"
        return
    fi

    # if the user asked for a diff, provide it
    if [ "$tsk_test_want_format" = "diff" ] && command -v diff &>/dev/null; then
        local diff_output="$(diff -u <(echo "$expected") <(echo "$actual") | tail -n +3 | cat -vet)"
        if [[ -n "$diff_output" ]]; then
            echo "diff:"$'\n'"$(_blockquote "$diff_output")"
            return
        fi
    fi

    echo "expected:"$'\n'"$(_blockquote "$expected")"$'\n'"    but got:"$'\n'"$(_blockquote "$actual")"
}

# todo? make line number and function name automatic?
_add_error() {
    tsk_test_messages+=("$1")
}

# same as _add_error, but includes a few lines of stderr after the message
_add_error_with_stderr() {
    local message="$1"
    local snippet="$(head -n 20 "$tsk_artifacts_dir/test-stderr")"
    message="$message"$'\n'"$(_blockquote "$snippet")"
    _add_error "$message"
}

_normalize_line_numbers() {
    # replace "line 12" with "line nnn"
    echo "$1" | sed -E 's/line [0-9]+:/line nnn:/g'
}

# used to test the API assertion calls: if an assertion failed,
# but was expected, then the failure is cleared and the test passes.
_expect_error() {
    local expected_message="$(_normalize_line_numbers "$1")"

    tsk_test_assertions=$((tsk_test_assertions + 1))

    # if nothing threw an error, that's an error.
    if [ ${#tsk_test_messages[@]} -eq 0 ]; then
        _add_error "line $(_line_number): _expect_error - expected error message: '$expected_message', but no errors were reported"
        return
    fi

    # Get the most recent error message
    local last_idx=$((${#tsk_test_messages[@]} - 1))
    local last_message="$(_normalize_line_numbers "${tsk_test_messages[last_idx]}")"

    if [ "$expected_message" = "$last_message" ]; then
        # the error was expected so it gets cleared
        unset "tsk_test_messages[last_idx]"
    else
        # the error we expected was not the error we got
        tsk_test_messages[last_idx]="line $(_line_number): _expect_error $(_format_diff "$expected_message" "$last_message")"
    fi
}

# at the end of the test, all mocks are removed
# TODO: one day we might want to set up mocks once and
# then run them in multiple tests.
_cleanup_mocks() {
    rm -rf "$tsk_artifacts_dir/test-mocks"/* "$tsk_artifacts_dir/test-mocks"/.*
}

_check_for_abandoned_files() {
    # Count all files (including hidden ones)
    local file_count=$(find . -mindepth 1 -maxdepth 1 | wc -l)

    # If any files found, add an error
    if [ "$file_count" -gt 0 ]; then
        local files_list=$(find . -mindepth 1 -maxdepth 1 -printf "%f\n" | sort)
        _add_error "test didn't clean up $(pluralize "$file_count" 'file'): $files_list"
    fi
}

_print_test_results() {
    local color="$GREEN"
    if [ $tsk_fail_count -eq 0 ]; then
        # if all tests pass, there's no need to keep the test directory
        rm -rf "$tsk_root_dir"
    fi

    local skip_msg=""
    if [ $tsk_skip_count -gt 0 ]; then
        color="$YELLOW"
        skip_msg=", ${tsk_skip_count} skipped"
    fi

    if [ $tsk_fail_count -gt 0 ]; then
        color="$RED"
    fi

    local count="${tsk_total_count} $(pluralize "$tsk_total_count" "test")"
    echo -e "${color}$count: ${tsk_pass_count} passed, ${tsk_fail_count} failed${skip_msg}.   ${tsk_total_assertions} assertions${NC}"
}

_find_test_directory() {
    local dir_num=1
    while [ -e "$tsk_orig_cwd/tsk-test-$(printf "%02d" $dir_num)" ]; do
        dir_num=$((dir_num + 1))
        if [ $dir_num -gt 99 ]; then
            echo "Too many test directories. Run: rm -r tsk-test-*" >&4
            exit 1
        fi
    done
    echo "$tsk_orig_cwd/tsk-test-$(printf "%02d" $dir_num)"
}

_run_tests() {
    local test

    cd "$tsk_run_dir" || exit 1
    for test in $(compgen -A function | grep '^test:'); do
        tsk_test_name="$test"
        tsk_test_messages=()
        tsk_test_assertions=0
        tsk_stderr_checked=false
        tsk_total_count=$((tsk_total_count + 1))
        tsk_test_want_format=""

        # in case the previous test redirected its output
        exec 1>"$tsk_artifacts_dir/test-stdout" 2>"$tsk_artifacts_dir/test-stderr"

        if [ -n "$CR" ]; then
            # no newline on this line, end_test will back up and print the result
            printf " %02d ....   $tsk_test_name" "$tsk_total_count" >&3
        fi

        "$test"

        # Add this test's assertions to the total
        tsk_total_assertions=$((tsk_total_assertions + tsk_test_assertions))

        _cleanup_mocks

        if [ -s "$tsk_artifacts_dir/test-stderr" ] && [ "$tsk_stderr_checked" = false ]; then
            _add_error_with_stderr "test produced stderr:"
        fi

        # TODO
        # _check_for_abandoned_files

        local res
        if [ "$tsk_total_count" -gt 0 ]; then
            # Check if the test made any assertions
            if [ $tsk_test_assertions -eq 0 ]; then
                # Test made no assertions, mark as skipped
                tsk_skip_count=$((tsk_skip_count + 1))
                res="${YELLOW}skip${NC}"
            elif [ ${#tsk_test_messages[@]} -eq 0 ]; then
                # Test passed (made assertions with no errors)
                tsk_pass_count=$((tsk_pass_count + 1))
                res="${GREEN}pass${NC}"
            else
                # test had one or more failing assertions
                tsk_fail_count=$((tsk_fail_count + 1))
                res="${RED}FAIL${NC}"

                # Since the test failed, rename the run directory to preserve the state
                local safe_name=$(echo "$tsk_test_name" | tr ' /\\:*?"<>|' '_')
                local failed_dir="$tsk_root_dir/failed-$safe_name"
                mv "$tsk_run_dir" "$failed_dir"

                # Move artifacts to the failed test directory with clear names
                mkdir -p "$failed_dir"
                mv "$tsk_artifacts_dir/test-stdout" "$failed_dir/FAILED-stdout.txt" 2>&4
                mv "$tsk_artifacts_dir/test-stderr" "$failed_dir/FAILED-stderr.txt" 2>&4
                if [ -d "$tsk_artifacts_dir/test-mocks" ]; then
                    mv "$tsk_artifacts_dir/test-mocks" "$failed_dir/FAILED-mocks" 2>&4
                fi
            fi

            local errmsg=''
            local num_fails="${#tsk_test_messages[@]}"
            if [ "$num_fails" -gt 0 ]; then
                errmsg="($num_fails $(pluralize "$num_fails" "error"))"
            elif [ $tsk_test_assertions -gt 0 ]; then
                errmsg="($tsk_test_assertions $(pluralize "$tsk_test_assertions" "assertion"))"
            fi

            printf "$CR %02d $res   $tsk_test_name $errmsg\n" "$tsk_total_count" >&3

            if [ ${#tsk_test_messages[@]} -gt 0 ]; then
                for msg in "${tsk_test_messages[@]}"; do
                    echo "  - $msg" >&3
                done
            fi
        fi
    done
}


tsk_total_count=0
tsk_pass_count=0
tsk_fail_count=0
tsk_skip_count=0
tsk_total_assertions=0

tsk_test_name=""
tsk_test_assertions=0

# TODO: ensure local directory is writeable first, bounce to /tmp if not
# TODO: option to create and mount a ramdisk to run tests

# Save current directory and PATH
tsk_orig_cwd="$(pwd)"
tsk_orig_path="$PATH"

tsk_root_dir="$(_find_test_directory)"
tsk_run_dir="$tsk_root_dir/run"
tsk_artifacts_dir="$tsk_root_dir/artifacts"
mkdir -p "$tsk_run_dir" "$tsk_artifacts_dir" 2>&4 || exit 1

_run_tests

# Restore original stdout and stderr, etc
exec 1>&3 2>&4
export PATH="$tsk_orig_path"
cd "$tsk_orig_cwd"

_print_test_results
